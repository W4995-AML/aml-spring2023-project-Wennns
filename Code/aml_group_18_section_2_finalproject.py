# -*- coding: utf-8 -*-
"""AML_Group_18_Section_2_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x_AcYp5NsyqsgEUWd6ALWTRWAGOyjCP2
"""

!pip install category_encoders

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, StandardScaler
warnings.filterwarnings('ignore')

!ls

from google.colab import drive
drive.mount('/content/drive/')

!ls

loan_data = data = pd.read_excel('/content/drive/MyDrive/AML_FinalProject_Group18/Bank_Personal_Loan_Modelling.xlsx','Data')

"""# Exploratory Data Analysis"""

loan_data.head()

loan_data.shape

loan_data.info()

"""## Data Analysis"""

edu_exp = loan_data.groupby("Education").Experience.mean().sort_values(ascending=True)
edu_exp

age_exp = loan_data.groupby('Age')['Experience'].mean().sort_values(ascending=False).reset_index()
age_exp

plt.figure(figsize=(10,7))
sns.lmplot(x="Age", y="Experience", data=age_exp)
plt.ylabel("Experience (Average)")
plt.title("Average of Experience by Age")

"""<p>Notice there are inconsistent data in our dataset. The number of years of experience couldn't be a negative number. After exploring the dataset, we found there are only few entries containing negative years of experience. Removing few entries will not influence our later model and prediction. So we'll remove these entries in data preprocessing part. </p>"""

summ = loan_data.describe()
pd.DataFrame(summ, columns=summ.columns).transpose()

"""## Feature Correlation Analysis"""

corr = loan_data.corr()
plt.subplots(figsize=(10,10));
sns.heatmap(corr, annot=True, square=True)
plt.title("Feature Correlation Matrix")
plt.tight_layout()

plt.figure(figsize=(16,5))
corr["Personal Loan"].sort_values(ascending=True)[:-1].plot(kind="barh")
plt.title("Correlation of features to Personal Loan\n", fontsize=15)
plt.xlabel("\nCorrelation to Personal Loan")
plt.tight_layout()
plt.show()

"""Analysis:

According to the correlation heatmap and the bar graph we plot above, we have the following initial observations:

1. The top 3 most important features to Personal_Loan is "Income", "CCAvg", and "CD Account"
2. "ZIP Code" have no correlation with our target variable "Personal_Loan".
3. From the Data Analysis part and the correlation heatmap, we can find "Age" and "Experience" are highly correlated and almost linearly related to each other, with correlation 0.99 with each other. Thus, we processing our data for model training, we can consider this pair of features as "Multicollinearity" and drop either one of them to avoid some negative influence on our model.

## Univariate Analysis

### Numerical Feature Distribution

#### Mortgage
"""

sns.distplot(loan_data["Mortgage"])
plt.title("Mortgage Distribution with KDE")

"""According to the plot above, we can find this plot of "Mortgage" is skewed to the right, meaning most people have no or only little mortgage. From this observation, we know we need to transform/standardize/normalize this feature for later data processing to avoid bias/distortation in our prediction results.

#### Experience
"""

sns.distplot(loan_data['Experience'])
plt.title("Distribution of Experience")

"""#### Income"""

fig, axes = plt.subplots(1, 2, figsize=(10,5))
sns.distplot(loan_data['Income'], ax=axes[0])
axes[0].title.set_text("The distribution plot for Income")
sns.boxplot(loan_data['Income'], orient="v", ax=axes[1])
axes[1].title.set_text("The box plot for Income")
plt.tight_layout()

"""Notice that the "Income" feature is also a little bit right skewed. From its box plot, we can find there are some outliers. So we also need to normalize this feature when feeding data into our model.

#### CCAvg
"""

fig, axes = plt.subplots(1, 2, figsize=(10,5))
sns.distplot(loan_data['CCAvg'], ax=axes[0])
sns.boxplot(loan_data['CCAvg'], orient="v", ax=axes[1])
axes[0].title.set_text("The distribution plot for CCAvg")
axes[1].title.set_text("The box plot for CCAvg")
plt.tight_layout()

"""According to the distribution and the box plot above, we can also find "CCAvg" is also right skewed. Moreover, outliers exist in the box plot. So we need to do normalization to avoid data imbalance issues.

### Categorical Feature Analysis

#### Education
"""

sns.distplot(loan_data['Education'])
plt.title("Distribution of Education")

"""#### Personal Loan"""

loan_data["Personal Loan"].value_counts().plot(kind='bar')

fig1, ax1 = plt.subplots()
explode = (0, 0.15)
ax1.pie(loan_data["Personal Loan"].value_counts(), explode=explode, labels=["0 : Didn't Buy", "1 : Bought"], autopct='%1.1f%%',
        shadow=True, startangle=70)
ax1.axis('equal')
plt.title("Personal Loan Percentage")
plt.show()

"""According to the pie chart above, we can find out of all customers, only 9.6% customers bought loan in the last campaign. So data imbalance issues exist in our dataset. We need to deal with this imbalance issue in the later part.

#### Family
"""

sns.distplot(loan_data['Family'])
plt.title("Distribution of Family")

"""## Bivariate Analysis

### Income VS. Peronal Loan
"""

plt.figure(figsize=(10,6))
sns.distplot(loan_data[loan_data["Personal Loan"] == 0]['Income'], color = 'r',label='Personal Loan=0',kde=True)
sns.distplot(loan_data[loan_data["Personal Loan"] == 1]['Income'], color = 'b',label='Personal Loan=1',kde=True)
plt.legend()
plt.title("Income Distribution", fontsize=15)

"""According to the distrubition graph between income and personal loan, we can find people who has personal load usually have higher income. For people whose income is lower, they are less likely to have personal loan.

### CCAvg VS. Personal Loan
"""

plt.figure(figsize=(10,6))
sns.distplot(loan_data[loan_data["Personal Loan"] == 0]['CCAvg'], color = 'r',label='Personal Loan=0',kde=True)
sns.distplot(loan_data[loan_data["Personal Loan"] == 1]['CCAvg'], color = 'b',label='Personal Loan=1',kde=True)
plt.legend()
plt.title("CCAvg Distribution")

"""According to the distrubition graph between CCAvg and personal loan, we can find people who has personal load usually have higher monthly spending with the credit card. For people whose income is lower, they spend less with the credit card each month.

### Age & Experience VS. Personal Loan
"""

plt.figure(figsize=(10,6))
sns.distplot(loan_data[loan_data["Personal Loan"] == 0]['Age'], color = 'r',label='Personal Loan=0',kde=True)
sns.distplot(loan_data[loan_data["Personal Loan"] == 1]['Age'], color = 'b',label='Personal Loan=1',kde=True)
plt.legend()
plt.title("Age Distribution")

plt.figure(figsize=(10,6))
sns.distplot(loan_data[loan_data["Personal Loan"] == 0]['Experience'], color = 'r',label='Personal Loan=0',kde=True)
sns.distplot(loan_data[loan_data["Personal Loan"] == 1]['Experience'], color = 'b',label='Personal Loan=1',kde=True)
plt.legend()
plt.title("Experience Distribution")

"""Based on our previous conclusion, we know age and year of experience are closed positively related to each other. So their distribution graphs show roughly the same pattern. Moreover, the distribution are kind of average for these two features vs. personal loan. Thus, these two features might not influence our target variable too much.

### Securities Account VS. Personal Loan
"""

plt.figure(figsize=(9,6))
sns.countplot(x="Securities Account", hue="Personal Loan", data=loan_data)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.title('Securities Account Countplot', fontsize=15)
plt.xlabel('Securities Account', fontsize=15)
plt.ylabel('Count', fontsize=15)

"""From the count plot above, we can find most people do not have securities account. Moreover, out of which, most people do not have personal loans either.

### Family Countplot VS. Personal Loan
"""

plt.figure(figsize=(9,6))
sns.countplot(x="Family", hue="Personal Loan", data=loan_data)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.title('Family Countplot', fontsize=15)
plt.xlabel('Family', fontsize=15)
plt.ylabel('Count', fontsize=15)

"""From the countplot above, we can find from the person who do not have personal loan, their family size tend to be 1, followed by 2. This is a reasonable observation sincc their expense is less than other categories of customers. Then for people who have personal loan, they usually have a larger family size. """

fig1, ax1 = plt.subplots()
explode = (0, 0.15)
# count the number of occurrences of each education level
counts = loan_data['Family'].value_counts()
ax1.pie(counts, labels=counts.index, autopct='%1.1f%%',
        shadow=True, startangle=70)
ax1.axis('equal')
plt.title("Family Percentage")
plt.show()

"""### Education VS. Personal Loan"""

plt.figure(figsize=(9,6))
sns.countplot(x="Education", hue="Personal Loan", data=loan_data)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.title('Education Countplot', fontsize=15)
plt.xlabel('Education', fontsize=15)
plt.ylabel('Count', fontsize=15)

fig1, ax1 = plt.subplots()
explode = (0, 0.15)
# count the number of occurrences of each education level
counts = loan_data['Education'].value_counts()
ax1.pie(counts, labels=["1 : Advanced/Professional", "2 : Graduate", "3: Undergrad"], autopct='%1.1f%%',
        shadow=True, startangle=70)
ax1.axis('equal')
plt.title("Education Percentage")
plt.show()

"""Customers with lower educational qualification are more likely to buy personal loan. As they should have a lower income. But customers who have high qualified education background are less prone to buy personal loan.

### Online VS. Personal Loan
"""

plt.figure(figsize=(9,6))
sns.countplot(x="Online", hue="Personal Loan", data=loan_data)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.title('Online Countplot', fontsize=15)
plt.xlabel('Online', fontsize=15)
plt.ylabel('Count', fontsize=15)

"""Most customers has online internet banking facility and the people who have these facility have bought more number of personal loans.

### Credit Card VS. Personal Loan
"""

plt.figure(figsize=(9,6))
sns.countplot(x="CreditCard", hue="Personal Loan", data=loan_data)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.title('CreditCard Countplot', fontsize=15)
plt.xlabel('CreditCard', fontsize=15)
plt.ylabel('Count', fontsize=15)

"""Most customers don't have a credit card. And the maximum number of customers who bought personal loan falls in this category, as they don't have the privilage to buy anything on credit using credit card. So they are more likely to buy things with personal loan.

### CD Account VS. Personal Loan
"""

plt.figure(figsize=(9,6))
sns.countplot(x="CD Account", hue="Personal Loan", data=loan_data)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.title('CD Account Countplot', fontsize=15)
plt.xlabel('CD Account', fontsize=15)
plt.ylabel('Count', fontsize=15)



"""### Income VS. Mortgage"""

plt.figure(figsize=(9,6))
plt.scatter(x=loan_data["Income"], y=loan_data["Mortgage"])
plt.title("Scatter Plot between Income and Mortgage", fontsize=20)
plt.xlabel("Income", fontsize=15)
plt.ylabel("Mortgage", fontsize=15)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.tight_layout()

"""Many of the customers don't have any mortgage in their name. So we can observe a line along the mortgage value zero. And the mortgage value for the customers increases with increase in their income.

### Income VS. CCAvg
"""

plt.figure(figsize=(9,6))
plt.scatter(x=loan_data["Income"], y=loan_data["CCAvg"])
plt.title("Scatter Plot between Income and CCAvg", fontsize=20)
plt.xlabel("Income", fontsize=15)
plt.ylabel("CCAvg", fontsize=15)
plt.xticks(horizontalalignment='center')
plt.yticks(horizontalalignment='right')
plt.tight_layout()

"""# Data Preprocessing

## Data Cleaning

1. After the initial data exploration, we found that the dataset doesn’t contain any missing or duplicate values.

2. Based on the Univariate Analysis, we need to drop all the noise records whose ‘Experience’ <0 or ‘ZIP Code’ < 5 digits.

3. Drop The variable ID, since it only acts as a customer identifier without adding any useful information to the dataset.

4. Drop ‘Age’ (strongly correlated with Experience.)
"""

# No missing Values
loan_data.isnull().sum().sum()

# No duplicated Values
loan_data[loan_data.duplicated(keep=False)].sum().sum()

loan_data[loan_data['Experience']<0]['Experience'].value_counts()

loan_data[loan_data['ZIP Code']<10000]

# Drop ‘Experience’ <0 or ‘ZIP Code’ < 5 digits.
loan_data = loan_data[loan_data['Experience'] >=0]

loan_data = loan_data[loan_data['ZIP Code'] >= 10000]

# Drop The variable ID, since it only acts as a customer identifier without adding any useful information to the dataset.
loan_data.drop(columns = {'ID'}, inplace = True)

# Drop ‘Age’ (strongly correlated with Experience.)
loan_data.drop(columns = {'Age'}, inplace = True)

loan_data.info()

"""## Feature Enginneering

1. Log transformation will be applied to remove the right skewness in ‘Income’, ‘‘CCAvg’, and ‘Mortgage’(too many zeros and plus one before log transformation).

2. Convert ‘CCAvg’ average monthly credit card spending to annual spending by times 12 (Equal the unit in the Income)

3. Category Encoding will be selected to convert ‘Family’, ‘Education’ and ‘Zip Code’ into numerical values.

"""

# Log transformation removes the right skewness in ‘Income’, ‘‘CCAvg’, and ‘Mortgage’
# (too many zeros and plus one before log transformation)

for i in ['Income', 'CCAvg', 'Mortgage']:
    loan_data[i] = (loan_data[i] + 1).apply(np.log10)

# Convert ‘CCAvg’ average monthly credit card spending to annual spending by times 12 
loan_data['CCAvg'] = loan_data['CCAvg']*12

loan_data

# Category Encoding on ‘Family’, ‘Education’ and ‘Zip Code’ into numerical values
import category_encoders as ce
from category_encoders import TargetEncoder

loan_data_X = loan_data.drop(columns = ['Personal Loan'])
loan_data_y = loan_data['Personal Loan']

te_features = ['Family', 'Education', 'ZIP Code']
te_df = ce.TargetEncoder(cols = te_features).fit_transform(loan_data_X, loan_data_y)

te_df = pd.concat((te_df, loan_data_y) , axis = 1)
te_df.head(5)

te_df.info()

"""## Address Imbalanced Data

1. Target Variable: Personal Loan (Whether the potential customers have a higher probability of purchasing the loan)

2. Only 9.6% customers bought loan in the last campaign

3. Synthetic Minority Oversampling Technique & Train-Test Split (80:20)- Reduce Overfitting incurred by Oversampling

4. After All the above steps:
    1. The shape of my development set is (3957, 11) 
    2. The shape of my test set is (990, 11)
    3. The shape of my development set after SMOTE is (5588, 11)
    4. The shape of my development target after SMOTE is (5588,)
    5. The number of positive labels in the development set are 2794
    6. The number of negative labels in the development set are 2794


"""

from sklearn.model_selection import train_test_split
X = te_df.drop(columns = 'Personal Loan')
y = te_df['Personal Loan']

X_dev, X_test, y_dev, y_test = train_test_split(X,y, test_size=0.2,
                                                stratify = y, random_state=42)
print('The shape of my development set is {}'.format(X_dev.shape) )
print('The shape of my test set is {}'.format(X_test.shape) )
print("*****************************************************************")
print('The shape of my development target is {}'.format(y_dev.shape) )
print('The shape of my test target is {}'.format(y_test.shape) )

te_df['Personal Loan'].value_counts(normalize = True)

y_dev.value_counts(normalize = True)

y_test.value_counts(normalize = True)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_dev_subsample_smote, y_dev_subsample_smote = smote.fit_resample(X_dev,y_dev)
print('The shape of my development set after SMOTE is {}'.format(X_dev_subsample_smote.shape))
print('The shape of my development target after SMOTE is {}'.format(y_dev_subsample_smote.shape))
print("The number of positive labels in the development set are {}".format(y_dev_subsample_smote.value_counts()[1]))
print("The number of negative labels in the development set are {}".format(y_dev_subsample_smote.value_counts()[0]))

"""# ML Techniques"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score
from sklearn.metrics import classification_report, RocCurveDisplay, ConfusionMatrixDisplay
from sklearn.svm import SVC

"""To train machine learning models for classifying personal loans, we will define several functions that help select best hyperparameters for each model."""

def f1_metric(model, X_train, y_train):
    return f1_score(y_train, model.predict(X_train), average='binary')

def tune_hyperparameters(MLclf, hyper_params, X_train, y_train):
    cross_validation = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)
    MLclf_search = GridSearchCV(MLclf, hyper_params, cv=cross_validation, scoring=f1_metric, n_jobs=-1)
    MLclf_search.fit(X_train, y_train)
    print("Best hyperparameters based on the 5-fold corss-validation:\n", MLclf_search.best_params_)    
    return MLclf_search.best_estimator_

"""## Logistic Regression Model

Logistic Regression ML classifier:

To train logistric regression classifier, we will follow the below steps:
1.   Train and test the classifier using the dataset after SMOTE method
2.   Visulize the confusion metric and ROC plot of the trained classifer in test dataset


3.   Train and test the classifier using the dataset without using SMOTE method
4.   Visulize the confusion metric and ROC plot of the trained classifer in test dataset

### With SMOTE Technique
Training Dataset With SMOTE Technique
"""

hyper_params = [{'solver':['saga'], 'penalty':['l1','l2','elasticnet'], 'C': np.logspace(-1, 1, 10), 'l1_ratio': np.arange(0,0.4,0.1)},
                {'solver':['lbfgs','liblinear','newton','saga'], 'penalty': ['l1','l2'], 'C': np.logspace(-1, 1, 10), 'l1_ratio': np.arange(0,0.4,0.1)}]   
logistic = LogisticRegression(max_iter=666)
logistic_smote_opt = tune_hyperparameters(logistic, hyper_params, X_dev_subsample_smote, y_dev_subsample_smote)

y_pred_train_smote = logistic_smote_opt.predict(X_dev_subsample_smote)
print("Classification performance (logistic regression) for SMOTE-based training dataset")
print("-"*60)
print(classification_report(y_dev_subsample_smote, y_pred_train_smote))

y_pred_test_smote = logistic_smote_opt.predict(X_test)
print('')
print("Classification performance (logistic regression) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_smote))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))
ConfusionMatrixDisplay.from_estimator(logistic_smote_opt, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (Logistic + SMOTE)')                                     

# ROC Curve 
RocCurveDisplay.from_estimator(logistic_smote_opt, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (Logistic + SMOTE)')
ax2.set_xlabel('False Positive')
ax2.set_ylabel('True Positive')
plt.show()

"""### Without SMOTE Technique"""

hyper_params = [{'solver':['saga'], 'penalty':['l1','l2','elasticnet'], 'C': np.logspace(-1, 1, 10), 'l1_ratio': np.arange(0,0.4,0.1)},
               {'solver':['lbfgs','liblinear','newton','saga'], 'penalty': ['l1','l2'], 'C': np.logspace(-1, 1, 10), 'l1_ratio': np.arange(0,0.4,0.1)}]   
logistic_ori = LogisticRegression(max_iter=666)
logistic_ori_opt = tune_hyperparameters(logistic_ori, hyper_params, X_dev, y_dev)

y_pred_train_ori = logistic_ori_opt.predict(X_dev)
print("Classification performance (logistic regression) for training dataset without SMOTE technique")
print("-"*60)
print(classification_report(y_dev, y_pred_train_ori))

y_pred_test_ori = logistic_ori_opt.predict(X_test)
print('')
print("Classification performance (logistic regression) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_ori))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))
ConfusionMatrixDisplay.from_estimator(logistic_ori_opt, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (Logistic)')                                     

# ROC Curve    
RocCurveDisplay.from_estimator(logistic_ori_opt, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (Logistic)')
ax2.set_xlabel('False Positive Rate')
ax2.set_ylabel('True Positive Rate')
plt.show()

"""## Random Forest Model

Random Forest ML classifier:

To train Random Forest classifier, we will follow the below steps:
1.   Train and test the RF classifier using the SMOTE-based dataset
2.   Visulize the confusion metric and ROC plot of the trained RF classifer in test dataset


3.   Train and test the RF classifier using the dataset without SMOTE technique
4.   Visulize the confusion metric and ROC plot of the trained classifer in test dataset

### With SMOTE Technique

SMOTE-based Dataset (Random Forest)
"""

hyper_params = {'max_depth': np.arange(4, 10), 'n_estimators': [10, 20, 50, 100], 'min_samples_split': [2, 3, 4], 'min_samples_leaf': [1, 2, 3]}
rf_smote = RandomForestClassifier(criterion='gini', random_state=0)
rf_opt_smote = tune_hyperparameters(rf_smote, hyper_params, X_dev_subsample_smote, y_dev_subsample_smote)

y_pred_train_smote = rf_opt_smote.predict(X_dev_subsample_smote)
print("Classification performance (Random Forest) for SMOTE-based training dataset")
print("-"*60)
print(classification_report(y_dev_subsample_smote, y_pred_train_smote))

y_pred_test_smote = rf_opt_smote.predict(X_test)
print('')
print("Classification performance (Random Forest) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_smote))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 3))
ConfusionMatrixDisplay.from_estimator(rf_opt_smote, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (Random Forest + SMOTE)')                                     

# ROC Curve 
RocCurveDisplay.from_estimator(rf_opt_smote, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (Random Forest + SMOTE)')
ax2.set_xlabel('False Positive')
ax2.set_ylabel('True Positive')
plt.show()

"""### Without SMOTE Technique
Dataset without using SMOTE technique (Random Forest)
"""

hyper_params = {'max_depth': np.arange(4, 10), 'n_estimators': [20, 50, 100], 'min_samples_split': [2, 3, 4], 'min_samples_leaf': [1, 2, 3]}
rf_ori = RandomForestClassifier(criterion='gini', random_state=0)
rf_opt_ori = tune_hyperparameters(rf_ori, hyper_params, X_dev, y_dev)

y_pred_train_ori = rf_opt_ori.predict(X_dev)
print("Classification performance (Random Forest) for training dataset without SMOTE technique")
print("-"*60)
print(classification_report(y_dev, y_pred_train_ori))

y_pred_test_ori = rf_opt_ori.predict(X_test)
print('')
print("Classification performance (Random Forest) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_ori))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))
ConfusionMatrixDisplay.from_estimator(rf_opt_ori, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (Random Forest)')                                     

# ROC Curve 
RocCurveDisplay.from_estimator(rf_opt_ori, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (Random Forest)')
ax2.set_xlabel('False Positive')
ax2.set_ylabel('True Positive')
plt.show()

"""## XGBoost Model

XGBoost ML classifier:

To train XGBoost classifier, we will follow the below steps:

1. Train and test the XGBoost classifier using the SMOTE-based dataset
2. Visulize the confusion metric and ROC plot of the trained *XGBoost* classifer in test dataset
3. Train and test the XGBoost classifier using the dataset without SMOTE technique
4. Visulize the confusion metric and ROC plot of the trained classifer in test dataset

### With SMOTE Technique
SMOTE-based Dataset
"""

hyper_params = {'learning_rate': [0.05, 0.1, 0.2], 'n_estimators': [30, 50],'subsample': [0.6, 0.8, 1.0],'reg_alpha': [0, 0.1, 1],'reg_lambda': [0, 0.1, 1]}
xgb_smote = XGBClassifier(random_state=0)
xgb_opt_smote = tune_hyperparameters(xgb_smote, hyper_params, X_dev_subsample_smote, y_dev_subsample_smote)

y_pred_train_smote = xgb_opt_smote.predict(X_dev_subsample_smote)
print("Classification performance (XGBoost) for SMOTE-based training dataset")
print("-"*60)
print(classification_report(y_dev_subsample_smote, y_pred_train_smote))

y_pred_test_smote = xgb_opt_smote.predict(X_test)
print('')
print("Classification performance (XGBoost) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_smote))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))
ConfusionMatrixDisplay.from_estimator(xgb_opt_smote, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (XGBoost+SMOTE)')                                     

# ROC Curve 
RocCurveDisplay.from_estimator(xgb_opt_smote, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (XGBoost+SMOTE)')
ax2.set_xlabel('False Positive')
ax2.set_ylabel('True Positive')
plt.show()

"""### Without SMOTE Technique
Dataset without using SMOTE technique
"""

hyper_params = {'learning_rate': [0.05, 0.1, 0.2], 'n_estimators': [30, 50],'subsample': [0.6, 0.8, 1.0],'reg_alpha': [0, 0.1, 1],'reg_lambda': [0, 0.1, 1]}
xgb_ori = XGBClassifier(random_state=0)
xgb_opt_ori = tune_hyperparameters(xgb_ori, hyper_params, X_dev, y_dev)

y_pred_train_ori = xgb_opt_ori.predict(X_dev)
print("Classification performance (XGBoost) for training dataset without SMOTE technique")
print("-"*60)
print(classification_report(y_dev, y_pred_train_ori))

y_pred_test_ori = xgb_opt_ori.predict(X_test)
print('')
print("Classification performance (XGBoost) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_ori))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))
ConfusionMatrixDisplay.from_estimator(xgb_opt_ori, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (XGBoost)')                                     

# ROC Curve 
RocCurveDisplay.from_estimator(xgb_opt_ori, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (XGBoost)')
ax2.set_xlabel('False Positive')
ax2.set_ylabel('True Positive')
plt.show()

"""## Support Vector Machine(SVM)

SVM ML classifier:

To train SVM classifier, we will follow the below steps:

1. Train and test the SVM classifier using the SMOTE-based dataset
2. Visulize the confusion metric and ROC plot of the trained SVM classifer in test dataset
3. Train and test the SVM classifier using the dataset without SMOTE technique
4. Visulize the confusion metric and ROC plot of the trained classifer in test dataset

### With SMOTE Technique
SMOTE-based Dataset
"""

hyper_params = [{'kernel': ['rbf'],'gamma': [1, 0.1, 0.01, 0.001], 'C': [100, 10, 0.1,1, 1000]}, {'kernel': ['linear'], 'C': [10, 0.1, 1, 100, 1000]}]
svm_smote = SVC(probability=True, max_iter=600)
svm_opt_smote = tune_hyperparameters(svm_smote, hyper_params, X_dev_subsample_smote, y_dev_subsample_smote)

y_pred_train_smote = svm_opt_smote.predict(X_dev_subsample_smote)
print("Classification performance (SVM) for SMOTE-based training dataset")
print("-"*60)
print(classification_report(y_dev_subsample_smote, y_pred_train_smote))

y_pred_test_smote = svm_opt_smote.predict(X_test)
print('')
print("Classification performance (SVM) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_smote))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))
ConfusionMatrixDisplay.from_estimator(svm_opt_smote, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (SVM+SMOTE)')                                     

# ROC Curve 
RocCurveDisplay.from_estimator(svm_opt_smote, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (SVM+SMOTE)')
ax2.set_xlabel('False Positive')
ax2.set_ylabel('True Positive')
plt.show()

"""### Without SMOTE Technique"""

hyper_params = [{'kernel': ['rbf'],'gamma': [1, 0.1, 0.01, 0.001], 'C': [0.1, 1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}]
svm_ori = SVC(probability=True, max_iter=600)
svm_opt_ori = tune_hyperparameters(svm_smote, hyper_params, X_dev, y_dev)

y_pred_train_ori = svm_opt_ori.predict(X_dev)
print("Classification performance (SVM) for training dataset without SMOTE technique")
print("-"*60)
print(classification_report(y_dev, y_pred_train_ori))

y_pred_test_ori = svm_opt_ori.predict(X_test)
print('')
print("Classification performance (SVM) for test dataset")
print("-"*60)
print(classification_report(y_test, y_pred_test_ori))

# Confusion Matrix
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))
ConfusionMatrixDisplay.from_estimator(svm_opt_ori, X_test, y_test, colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix for Test Data (SVM)')                                     

# ROC Curve 
RocCurveDisplay.from_estimator(svm_opt_ori, X_test, y_test, ax=ax2)
ax2.set_title('ROC Curve for Test Data (SVM)')
ax2.set_xlabel('False Positive')
ax2.set_ylabel('True Positive')
plt.show()